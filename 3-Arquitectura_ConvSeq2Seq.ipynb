{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "import utils ##\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audios = pd.read_csv('datos_procesados/valid_samples.csv', index_col=0)\n",
    "audio_names = list(df_audios.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definición del objeto Dataset para importar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarareoMIDIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tarareo_directory: str, midi_directory: str, samples: list, extensions: tuple[str, str]):\n",
    "        self._tarareo_directory = tarareo_directory\n",
    "        self._midi_directory = midi_directory\n",
    "        \n",
    "        self._samples = samples #  Nombre del elemento de grabación\n",
    "        self._extensions = extensions #  extension del nombre del tarareo y vector midi. Ejm audio_name[extension] (incluye formato)\n",
    "\n",
    "        # Si usamos PADDING, será conveniente tener un vocabulario como si se tratara de una traducción en NLP\n",
    "        # Esto ayuda a ordenar las etiquetas por orden de frecuencia\n",
    "        # Colocando en las primeras casillas a las etiquetas de inicio y final\n",
    "        # self._vocabulary = vocabulary\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Recuperar nombre de la muestra\n",
    "        file_name = self._samples[idx]\n",
    "        tarareo_name = file_name + self._extensions[0]\n",
    "        midi_name = file_name + self._extensions[1]\n",
    "\n",
    "        # Importar\n",
    "        tarareo_path = os.path.join(self._tarareo_directory, tarareo_name)\n",
    "        tarareo = np.load(tarareo_path)\n",
    "\n",
    "        midi_path = os.path.join(self._midi_directory, midi_name)\n",
    "        midi = np.load(midi_path)\n",
    "\n",
    "        # Agregar <SOS> y <EOS> ?\n",
    "        return torch.tensor(tarareo, dtype=torch.float32), torch.tensor(midi, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definición de `collate_fn` para crear batches y homologar las dimensiones de audios de distinta duración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_vector_collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Función para agregar padding a los audios y vectores de etiquetas por lote.\n",
    "\n",
    "    :param batch: Una lista de tuplas donde cada tupla contiene un audio y su vector de etiquetas.\n",
    "    :return: Una tupla que contiene dos tensores:\n",
    "             1) Un tensor que contiene todos los audios del lote, apilados juntos. Forma: [batch_size, L_max, N_max].\n",
    "             2) Un tensor que contiene todos los vectores de etiquetas del lote, rellenados con zeros para que tengan la misma longitud. \n",
    "                Forma: [batch_size, K_max].\n",
    "    \"\"\"\n",
    "    audios, labels = zip(*batch)\n",
    "    \n",
    "    # Determinar las dimensiones máximas\n",
    "    L_max = max(audio.size(0) for audio in audios)\n",
    "    N_max = max(audio.size(1) for audio in audios)\n",
    "    K_max = max(label.size(0) for label in labels)\n",
    "    \n",
    "    # Inicializar los tensores con padding\n",
    "    padded_audios = torch.zeros(len(audios), L_max, N_max, dtype=torch.float32)\n",
    "    padded_labels = torch.zeros(len(labels), K_max, dtype=torch.int64)\n",
    "    \n",
    "    for i, (audio, label) in enumerate(batch):\n",
    "        L = audio.size(0)\n",
    "        N = audio.size(1)\n",
    "        K = label.size(0)\n",
    "        \n",
    "        # Copiar el audio y el vector de etiquetas a los tensores con padding\n",
    "        padded_audios[i, :L, :N] = audio\n",
    "        padded_labels[i, :K] = label\n",
    "    \n",
    "    return padded_audios, padded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarareo_directory = \"datos_procesados/tarareos/ventanas/\"\n",
    "midi_directory = 'datos_procesados/midis/target_vectors/'\n",
    "extensions = ('_frames.npy', '_target.npy')\n",
    "dataset = TarareoMIDIDataset(tarareo_directory=tarareo_directory, midi_directory=midi_directory, samples=audio_names, extensions=extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [0.7, 0.2, 0.1]\n",
    "train_size = int(props[0] * len(dataset))\n",
    "val_size = int(props[1] * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, props) # [train_size, val_size, test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=audio_vector_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=audio_vector_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=audio_vector_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura ConvSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convout_calc(input_dim, kernel, stride, dilation=1):\n",
    "    new_dim = np.floor((input_dim - 1 - dilation*(kernel-1)) / stride + 1)\n",
    "    return int(new_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Conv2D \n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, \n",
    "                                      out_channels=1, \n",
    "                                      kernel_size=(int(1378//16), 4), \n",
    "                                      stride=(1, 1))\n",
    "        # Dimensión H de Conv2D\n",
    "        self.conv2_H_dim = convout_calc(input_dim, kernel=int(1378//16), stride=1)\n",
    "        \n",
    "        # MaxPooling2D\n",
    "        self.max_pool2d = torch.nn.MaxPool2d(kernel_size=(int(86//4), 2))\n",
    "        # Dimensión H de MaxPooling2D\n",
    "        self.pool_H_dim = convout_calc(self.conv2_H_dim, kernel=int(86//4), stride=int(86//4))\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = torch.nn.LSTM(input_size=self.pool_H_dim, \n",
    "                                          hidden_size=hidden_dim, \n",
    "                                          batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, L, N]\n",
    "        # x = x.unsqueeze(1) # Canal pivote\n",
    "\n",
    "        # ETAPA Convolucional\n",
    "        conv_out = self.conv2d(x)\n",
    "        pool_out = self.max_pool2d(conv_out)\n",
    "\n",
    "        # Transformar a dimensiones apropiadas para LSTM\n",
    "        pool_out = pool_out.squeeze(1)\n",
    "        pool_out_t = pool_out.transpose(1, 2)\n",
    "        \n",
    "        # ETAPA Recurrente\n",
    "        encoder_outputs, (hidden, cell) = self.encoder_lstm(pool_out_t)\n",
    "        \n",
    "        return encoder_outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, labels_dim, hidden_dim, embedding_dim, dropout=0.25):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Parámetros\n",
    "        self._labels_dim = labels_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._embedding_dim = embedding_dim\n",
    "\n",
    "        # Decoder LSTM\n",
    "        self.embedding = torch.nn.Embedding(labels_dim, embedding_dim)\n",
    "        self.decoder_lstm = torch.nn.LSTM(input_size=embedding_dim,\n",
    "                                          hidden_size=hidden_dim,\n",
    "                                          batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Capa lineal que mapeará las salidas del LSTM a la dimensión de salida (es decir, al número de clases)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, labels_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, cell): #, teacher_forcing_ratio=0.5, target_seq=None\n",
    "        # input : [batch_size]\n",
    "\n",
    "        # Embedding:\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        embedded = embedded.unsqueeze(1) # embedded: [batch_size, 1, embedding_dim]\n",
    "        \n",
    "        # Aplicar celda recurrente\n",
    "        output, (hidden, cell) = self.decoder_lstm(embedded, (hidden, cell)) # output: [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Predicción\n",
    "        output = output.squeeze(1) # output: [batch_size, hidden_dim]\n",
    "        logits = self.fc(output) # logits: [batch_size, labels_dim]\n",
    "\n",
    "        return logits, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [batch_size, L, N]\n",
    "        # trg = [batch_size, K]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_length = trg.shape[1]\n",
    "        labels_size = self.decoder._labels_dim\n",
    "\n",
    "        # Placeholder del tensor de salida\n",
    "        outputs = torch.zeros(batch_size, trg_length, labels_size).to(self.device)\n",
    "\n",
    "        # Último estado oculto del encoder\n",
    "        _, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        \n",
    "        input = trg[:, 0] # Se comienza en la primera celda del vector MIDI objetivo\n",
    "        # input: [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # Realiza la predicción del valor de celda\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch_size, labels_dim]\n",
    "            # hidden,cell = [batch size, hidden dim]\n",
    "\n",
    "            # Almacena los logits para la celda t\n",
    "            outputs[:,t,:] = output\n",
    "\n",
    "            # Decidir si se realiza teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                input = trg[:,t]\n",
    "                pass\n",
    "            else:\n",
    "                # Escoge el código más probable\n",
    "                top1 = output.argmax(1)\n",
    "                input = top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1378\n",
    "batch_size = 20\n",
    "N = 160\n",
    "K=12\n",
    "hidden_dim = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim=L, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(labels_dim=3, embedding_dim=3, hidden_dim=hidden_dim, dropout=0.25)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "x_src = torch.rand(batch_size,L,N)\n",
    "y_trg = torch.randint(0,3, (batch_size,K))\n",
    "_ = model(x_src, y_trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Función de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq2seq(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_loader:\n",
    "            src, trg = src.unsqueeze(1), trg  # Añadir dimensión de canal\n",
    "            output = model(src, trg, 0)  # No usar teacher forcing durante la evaluación\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for src, trg in train_loader:\n",
    "            src, trg = src.unsqueeze(1), trg  # Añadir dimensión de canal\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg)\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss = evaluate_seq2seq(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1378\n",
    "input_dim = L\n",
    "batch_size = 20\n",
    "hidden_dim = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=L, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(labels_dim=3, embedding_dim=3, hidden_dim=hidden_dim, dropout=0.25)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de optimizador y función de pérdida\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Ignorar el padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_seq2seq(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention, hidden_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = 3  # As there are 3 possible labels [0, 1, 2]\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            context_vector, _ = self.attention(encoder_outputs, hidden[-1])\n",
    "            input = input.unsqueeze(1)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de parámetros\n",
    "input_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Etiquetas [0, 1, 2]\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim)\n",
    "attention = Attention(hidden_dim)\n",
    "decoder = Decoder(output_dim, hidden_dim)\n",
    "model = Seq2SeqWithAttention(encoder, decoder, attention, hidden_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
