{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "import utils ##\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audios = pd.read_csv('datos_procesados/valid_samples.csv', index_col=0)\n",
    "audio_names = list(df_audios.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definición del objeto Dataset para importar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarareoMIDIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tarareo_directory: str, midi_directory: str, samples: list, extensions: tuple[str, str]):\n",
    "        self._tarareo_directory = tarareo_directory\n",
    "        self._midi_directory = midi_directory\n",
    "        \n",
    "        self._samples = samples #  Nombre del elemento de grabación\n",
    "        self._extensions = extensions #  extension del nombre del tarareo y vector midi. Ejm audio_name[extension] (incluye formato)\n",
    "\n",
    "        # Si usamos PADDING, será conveniente tener un vocabulario como si se tratara de una traducción en NLP\n",
    "        # Esto ayuda a ordenar las etiquetas por orden de frecuencia\n",
    "        # Colocando en las primeras casillas a las etiquetas de inicio y final\n",
    "        # self._vocabulary = vocabulary\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Recuperar nombre de la muestra\n",
    "        file_name = self._samples[idx]\n",
    "        tarareo_name = file_name + self._extensions[0]\n",
    "        midi_name = file_name + self._extensions[1]\n",
    "\n",
    "        # Importar\n",
    "        tarareo_path = os.path.join(self._tarareo_directory, tarareo_name)\n",
    "        tarareo = np.load(tarareo_path)\n",
    "        \n",
    "\n",
    "        midi_path = os.path.join(self._midi_directory, midi_name)\n",
    "        midi = np.load(midi_path)\n",
    "        \n",
    "\n",
    "        # Agregar <SOS> y <EOS> ? -> 0 \n",
    "        # Tarareo: Intensidad 0\n",
    "        # Midi: Silencio <-> 0\n",
    "        tara_zero = np.zeros((tarareo.shape[0],1))\n",
    "        midi_zero = np.array([0])\n",
    "        tarareo = np.hstack( (tara_zero,  tarareo, tara_zero) )\n",
    "        midi = np.hstack( (midi_zero,  midi, midi_zero) )\n",
    "\n",
    "\n",
    "        # Convertir a tensor\n",
    "        tarareo = torch.tensor(tarareo, dtype=torch.float32)\n",
    "        midi = torch.tensor(midi, dtype=torch.int)\n",
    "\n",
    "        return tarareo , midi #{'tarareo':tarareo , 'midi':midi}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definición de `collate_fn` para crear batches y homologar las dimensiones de audios de distinta duración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_vector_collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Función para agregar padding a los audios y vectores de etiquetas por lote.\n",
    "\n",
    "    :param batch: Una lista de tuplas donde cada tupla contiene un audio y su vector de etiquetas.\n",
    "    :return: Una tupla que contiene dos tensores:\n",
    "             1) Un tensor que contiene todos los audios del lote, apilados juntos. Forma: [batch_size, L_max, N_max].\n",
    "             2) Un tensor que contiene todos los vectores de etiquetas del lote, rellenados con zeros para que tengan la misma longitud. \n",
    "                Forma: [batch_size, K_max].\n",
    "    \"\"\"\n",
    "    audios, labels = zip(*batch)\n",
    "    \n",
    "    # Determinar las dimensiones máximas\n",
    "    L_max = max(audio.size(0) for audio in audios)\n",
    "    N_max = max(audio.size(1) for audio in audios)\n",
    "    K_max = max(label.size(0) for label in labels)\n",
    "    \n",
    "    # Inicializar los tensores con padding\n",
    "    padded_audios = torch.zeros(len(audios), L_max, N_max, dtype=torch.float32)\n",
    "    padded_labels = torch.zeros(len(labels), K_max, dtype=torch.int64)\n",
    "    \n",
    "    for i, (audio, label) in enumerate(batch):\n",
    "        L = audio.size(0)\n",
    "        N = audio.size(1)\n",
    "        K = label.size(0)\n",
    "        \n",
    "        # Copiar el audio y el vector de etiquetas a los tensores con padding\n",
    "        padded_audios[i, :L, :N] = audio\n",
    "        padded_labels[i, :K] = label\n",
    "    \n",
    "    return padded_audios, padded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separación de conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarareo_directory = \"datos_procesados/tarareos/ventanas/\"\n",
    "midi_directory = 'datos_procesados/midis/target_vectors/'\n",
    "extensions = ('_frames.npy', '_target.npy')\n",
    "dataset = TarareoMIDIDataset(tarareo_directory=tarareo_directory, midi_directory=midi_directory, samples=audio_names, extensions=extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [0.7, 0.2, 0.1]\n",
    "train_size = int(props[0] * len(dataset))\n",
    "val_size = int(props[1] * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, props) # [train_size, val_size, test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=audio_vector_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=audio_vector_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=audio_vector_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura ConvSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convout_calc(input_dim, kernel, stride, dilation=1):\n",
    "    new_dim = np.floor((input_dim - 1 - dilation*(kernel-1)) / stride + 1)\n",
    "    return int(new_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Conv2D \n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, \n",
    "                                      out_channels=1, \n",
    "                                      kernel_size=(int(1378//16), 4), \n",
    "                                      stride=(1, 1))\n",
    "        # Dimensión H de Conv2D\n",
    "        self.conv2_H_dim = convout_calc(input_dim, kernel=int(1378//16), stride=1)\n",
    "        \n",
    "        # MaxPooling2D\n",
    "        self.max_pool2d = torch.nn.MaxPool2d(kernel_size=(int(86//4), 2))\n",
    "        # Dimensión H de MaxPooling2D\n",
    "        self.pool_H_dim = convout_calc(self.conv2_H_dim, kernel=int(86//4), stride=int(86//4))\n",
    "        \n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = torch.nn.LSTM(input_size=self.pool_H_dim, \n",
    "                                          hidden_size=hidden_dim, \n",
    "                                          batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, L, N]\n",
    "        x = x.unsqueeze(1) # Canal pivote\n",
    "\n",
    "        # ETAPA Convolucional\n",
    "        conv_out = self.conv2d(x)\n",
    "        pool_out = self.max_pool2d(conv_out)\n",
    "\n",
    "        # Transformar a dimensiones apropiadas para LSTM\n",
    "        pool_out = pool_out.squeeze(1)\n",
    "        pool_out_t = pool_out.transpose(1, 2)\n",
    "        \n",
    "        # ETAPA Recurrente\n",
    "        encoder_outputs, (hidden, cell) = self.encoder_lstm(pool_out_t)\n",
    "        \n",
    "        return encoder_outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, labels_dim, hidden_dim, embedding_dim, dropout=0.25):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Parámetros\n",
    "        self._labels_dim = labels_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._embedding_dim = embedding_dim\n",
    "\n",
    "        # Decoder LSTM\n",
    "        self.embedding = torch.nn.Embedding(labels_dim, embedding_dim)\n",
    "        self.decoder_lstm = torch.nn.LSTM(input_size=embedding_dim,\n",
    "                                          hidden_size=hidden_dim,\n",
    "                                          batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Capa lineal que mapeará las salidas del LSTM a la dimensión de salida (es decir, al número de clases)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, labels_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, cell): #, teacher_forcing_ratio=0.5, target_seq=None\n",
    "        # input : [batch_size]\n",
    "\n",
    "        # Embedding:\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        embedded = embedded.unsqueeze(1) # embedded: [batch_size, 1, embedding_dim]\n",
    "        \n",
    "        # Aplicar celda recurrente\n",
    "        output, (hidden, cell) = self.decoder_lstm(embedded, (hidden, cell)) # output: [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Predicción\n",
    "        output = output.squeeze(1) # output: [batch_size, hidden_dim]\n",
    "        logits = self.fc(output) # logits: [batch_size, labels_dim]\n",
    "\n",
    "        return logits, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [batch_size, L, N]\n",
    "        # trg = [batch_size, K]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_length = trg.shape[1]\n",
    "        labels_size = self.decoder._labels_dim\n",
    "\n",
    "        # Placeholder del tensor de salida\n",
    "        outputs = torch.zeros(batch_size, trg_length, labels_size).to(self.device)\n",
    "\n",
    "        # Último estado oculto del encoder\n",
    "        _, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        \n",
    "        input = trg[:, 0] # Se comienza en la primera celda del vector MIDI objetivo\n",
    "        # input: [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # Realiza la predicción del valor de celda\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch_size, labels_dim]\n",
    "            # hidden,cell = [batch size, hidden dim]\n",
    "\n",
    "            # Almacena los logits para la celda t\n",
    "            outputs[:,t,:] = output\n",
    "\n",
    "            # Decidir si se realiza teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                input = trg[:,t]\n",
    "                pass\n",
    "            else:\n",
    "                # Escoge el código más probable\n",
    "                top1 = output.argmax(1)\n",
    "                input = top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1378\n",
    "batch_size = 20\n",
    "N = 160\n",
    "K=12\n",
    "hidden_dim = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim=L, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(labels_dim=3, embedding_dim=3, hidden_dim=hidden_dim, dropout=0.25)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "x_src = torch.rand(batch_size,L,N)\n",
    "y_trg = torch.randint(0,3, (batch_size,K))\n",
    "_ = model(x_src, y_trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1378\n",
    "input_dim = L\n",
    "batch_size = 20\n",
    "hidden_dim = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=L, hidden_dim=hidden_dim)\n",
    "decoder = Decoder(labels_dim=3, embedding_dim=3, hidden_dim=hidden_dim, dropout=0.25)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weight initializatión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (conv2d): Conv2d(1, 1, kernel_size=(86, 4), stride=(1, 1))\n",
       "    (max_pool2d): MaxPool2d(kernel_size=(21, 2), stride=(21, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder_lstm): LSTM(61, 30, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3, 3)\n",
       "    (decoder_lstm): LSTM(3, 30, batch_first=True)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (fc): Linear(in_features=30, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        torch.nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,807 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in (enumerate(tqdm(data_loader, desc=\"Training batches\"))):\n",
    "        src = batch[0].to(device)\n",
    "        trg = batch[1].to(device)\n",
    "        # src = [batch size, L, N]\n",
    "        # trg = [batch size, K]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [batch size, K, label_size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [batch size * (K - 1), label_size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [batch size * (K - 1)]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[0].to(device)\n",
    "            trg = batch[1].to(device)\n",
    "            # src = [batch size, L, N]\n",
    "            # trg = [batch size, K]\n",
    "            output = model(src, trg, 0)  # apaga teacher forcing\n",
    "            # output = [batch size, K, label_size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [batch size * (K - 1), label_size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [batch size * (K - 1)]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print('train -----------------------------------------------------------------------')\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    print('val -----------------------------------------------------------------------')\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"ConvSeq2Seq_model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 segundos / batch | 220 batches -> 1760 s (30 min / epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"ConvSeq2Seq_model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention, hidden_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = attention\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = 3  # As there are 3 possible labels [0, 1, 2]\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            context_vector, _ = self.attention(encoder_outputs, hidden[-1])\n",
    "            input = input.unsqueeze(1)\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de parámetros\n",
    "input_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # Etiquetas [0, 1, 2]\n",
    "\n",
    "encoder = Encoder(input_dim, hidden_dim)\n",
    "attention = Attention(hidden_dim)\n",
    "decoder = Decoder(output_dim, hidden_dim)\n",
    "model = Seq2SeqWithAttention(encoder, decoder, attention, hidden_dim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
